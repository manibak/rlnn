{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그림그리는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_v_table_small(v_table, env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1])\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1])\n",
    "\n",
    "# V table 그리기    \n",
    "def show_v_table(v_table, env):    \n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# Q table 그리기\n",
    "def show_q_table(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n",
    "                if k==1:\n",
    "                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "\n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_q_table_arrow(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,0]:\n",
    "                        print(\"        ↑       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==1:                    \n",
    "                    if np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←  →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,1]:\n",
    "                        print(\"          →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==2:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,2]:\n",
    "                        print(\"        ↓       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")    \n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy_small(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"road\":\n",
    "                if policy[i,j] == 0:\n",
    "                    print(\"   ↑     |\",end=\"\")\n",
    "                elif policy[i,j] == 1:\n",
    "                    print(\"   →     |\",end=\"\")\n",
    "                elif policy[i,j] == 2:\n",
    "                    print(\"   ↓     |\",end=\"\")\n",
    "                elif policy[i,j] == 3:\n",
    "                    print(\"   ←     |\",end=\"\")\n",
    "            else:\n",
    "                print(\"          |\",end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                    if policy[i,j] == 0:\n",
    "                        print(\"      ↑         |\",end=\"\")\n",
    "                    elif policy[i,j] == 1:\n",
    "                        print(\"      →         |\",end=\"\")\n",
    "                    elif policy[i,j] == 2:\n",
    "                        print(\"      ↓         |\",end=\"\")\n",
    "                    elif policy[i,j] == 3:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n",
    "    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "    \n",
    "    # 2. 각 행동별 선택확률\n",
    "    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    # 3. 에이전트의 초기 위치 저장\n",
    "    def __init__(self):\n",
    "        self.pos = (0,0)\n",
    "    \n",
    "    # 4. 에이전트의 위치 저장\n",
    "    def set_pos(self,position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "    \n",
    "    # 5. 에이전트의 위치 불러오기\n",
    "    def get_pos(self):\n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "    \n",
    "    # 2. 목적지 좌표 설정\n",
    "    goal_position = [2,2]\n",
    "    \n",
    "    # 3. 보상 리스트 숫자\n",
    "    reward_list = [[road,road,road],\n",
    "                   [road,road,road],\n",
    "                   [road,road,goal]]\n",
    "    \n",
    "    # 4. 보상 리스트 문자\n",
    "    reward_list1 = [[\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"goal\"]]\n",
    "    \n",
    "    # 5. 보상 리스트를 array로 설정\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)    \n",
    "\n",
    "    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n",
    "    def move(self, agent, action):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # 6.1 행동에 따른 좌표 구하기\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "        \n",
    "        # 6.2 현재좌표가 목적지 인지확인\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.3 이동 후 좌표가 미로 밖인 확인    \n",
    "        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.4 이동 후 좌표가 길이라면\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "            \n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 재귀적으로 상태 가치함수를 계산하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 가치 계산\n",
    "def state_value_function(env,agent,G,max_step,now_step):\n",
    "    \n",
    "    # 1. 감가율 설정\n",
    "    gamma = 0.9\n",
    "    \n",
    "# 2. 현재 위치가 도착지점인지 확인\n",
    "    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "        return env.goal\n",
    "    \n",
    "# 3. 마지막 상태는 보상만 계산\n",
    "    if (max_step == now_step):\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 3.1 가능한 모든 행동의 보상을 계산\n",
    "        for i in range(len(agent.action)):\n",
    "            agent.set_pos(pos1)\n",
    "            observation, reward, done = env.move(agent,i)\n",
    "            G += agent.select_action_pr[i] * reward\n",
    "            \n",
    "        return G\n",
    "    \n",
    "    # 4. 현재 상태의 보상을 계산한 후 다음 step으로 이동\n",
    "    else:\n",
    "        \n",
    "        # 4.1현재 위치 저장\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 4.2 현재 위치에서 가능한 모든 행동을 조사한 후 이동\n",
    "        for i in range(len(agent.action)):\n",
    "            observation, reward, done = env.move(agent,i)      \n",
    "            # 4.2.1 현재 상태에서 보상을 계산\n",
    "            G += agent.select_action_pr[i] * reward\n",
    "\n",
    "            # 4.2.2 이동 후 위치 확인 : 미로밖, 벽, 구멍인 경우 이동전 좌표로 다시 이동\n",
    "            if done == True:\n",
    "                if observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1]:\n",
    "                    agent.set_pos(pos1)\n",
    "\n",
    "            # 4.2.3 다음 step을 계산\n",
    "            next_v = state_value_function(env, agent, 0, max_step, now_step+1)\n",
    "            G += agent.select_action_pr[i] * gamma * next_v\n",
    "\n",
    "            # 4.2.4 현재 위치를 복구\n",
    "            agent.set_pos(pos1)\n",
    "\n",
    "        return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 𝜖−𝑔𝑟𝑒𝑒𝑑𝑦, 𝑔𝑟𝑒𝑒𝑑𝑦  함수 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  𝜖−𝑔𝑟𝑒𝑒𝑑𝑦, 𝑔𝑟𝑒𝑒𝑑𝑦  함수 작성\n",
    "def e_greedy(Q_table,agent,epsilon):\n",
    "    pos = agent.get_pos()\n",
    "    greedy_action = np.argmax(Q_table[pos[0],pos[1],:])\n",
    "    pr = np.zeros(4)\n",
    "    for i in range(len(agent.action)):\n",
    "        if i == greedy_action:\n",
    "            pr[i] = 1 - epsilon + epsilon/len(agent.action)\n",
    "        else:\n",
    "            pr[i] = epsilon / len(agent.action)\n",
    "\n",
    "    return np.random.choice(range(0,len(agent.action)), p=pr)    \n",
    "\n",
    "def greedy(Q_table,agent,epsilon):\n",
    "    pos = agent.get_pos()\n",
    "    return np.argmax(Q_table[pos[0],pos[1],:])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) coltrol : Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start TD(0) control : Q-learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10000/10000 [00:01<00:00, 5120.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning : Q(s,a)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      1.10       |      2.56       |      4.17       |\n",
      "|  1.10      4.56 |  3.10      6.17 |  4.55      4.17 |\n",
      "|      4.56       |      6.18       |      7.97       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      3.10       |      4.56       |      6.17       |\n",
      "|  2.56      6.18 |  4.56      7.98 |  6.18      5.97 |\n",
      "|      6.18       |      7.97       |      9.97       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      4.56       |      6.18       |      9.97       |\n",
      "|  4.17      7.97 |  6.17      9.97 |  9.97      9.97 |\n",
      "|      4.17       |      5.97       |      9.97       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Q-learning :optimal policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↓         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ←         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "#  TD(0) contro : Q-learning\n",
    "\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "np.random.seed(0)\n",
    "\n",
    "# 모든 𝑠∈𝑆,𝑎∈𝐴(𝑆)에 대해 초기화:\n",
    "# 𝑄(𝑠,𝑎)←임의의 값\n",
    "Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "\n",
    "# Q(𝑡𝑒𝑟𝑚𝑖𝑛𝑎𝑙−𝑠𝑡𝑎𝑡𝑒,𝑎)=0\n",
    "Q_table[2,2,:] = 0\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start TD(0) control : Q-learning\")\n",
    "alpha = 0.1\n",
    "epsilon = 0.8\n",
    "\n",
    "# 각 에피소드에 대해 반복 :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    dleta = 0\n",
    "    # S 를 초기화\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "\n",
    "    # 에피소드의 각 스텝에 대해 반복 :\n",
    "    for k in range(max_step):\n",
    "        # s 에서 행동 정책(Behavior policy)으로 행동 a를 선택 ( 예 : ε-greedy\n",
    "        pos = agent.get_pos()\n",
    "        action = e_greedy(Q_table,agent,epsilon)\n",
    "        # 행동 a 를 취한 후 보상 r과 다음 상태 s^'를 관측\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "\n",
    "        # s^' 에서 타깃 정책(Target policy)으로 행동 a^'를 선택 ( 예 : greedy)\n",
    "\n",
    "        next_action = greedy(Q_table,agent,epsilon)\n",
    "        \n",
    "        # Q(s,a)←Q(s,a) + α[r+𝛾  maxa'𝑄(s',a')−𝑄(s,a)] \n",
    "        Q_table[pos[0],pos[1],action] += alpha * (reward + gamma * Q_table[observation[0], observation[1],next_action] - Q_table[pos[0],pos[1],action])\n",
    "        # s가 마지막 상태라면 종료\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "# 학습된 정책에서 최적 행동 추출\n",
    "optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(Q_table[i,j,:])\n",
    "\n",
    "print(\"Q-learning : Q(s,a)\")\n",
    "show_q_table(np.round(Q_table,2),env)\n",
    "print(\"Q-learning :optimal policy\")\n",
    "show_policy(optimal_policy,env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "    \n",
    "    # 2. 목적지 좌표 설정\n",
    "    goal_position = [2,2]\n",
    "    \n",
    "    # 3. 보상 리스트 숫자\n",
    "    reward_list = [[road,road,road],\n",
    "                   [road,road,road],\n",
    "                   [road,road,goal]]\n",
    "    \n",
    "    # 4. 보상 리스트 문자\n",
    "    reward_list1 = [[\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"goal\"]]\n",
    "    \n",
    "    # 5. 보상 리스트를 array로 설정\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)    \n",
    "\n",
    "    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n",
    "    def move(self, agent, action):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # 6.1 행동에 따른 좌표 구하기\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "        \n",
    "        # 6.2 현재좌표가 목적지 인지확인\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.3 이동 후 좌표가 미로 밖인 확인    \n",
    "        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.4 이동 후 좌표가 길이라면\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "            \n",
    "        return observation, reward, done"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da3341483761adb085defe5ac4650c29f9bcd064904b4d09c86168e7130966c4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
